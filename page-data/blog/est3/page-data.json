{"componentChunkName":"component---src-components-blogpost-blogpost-jsx","path":"/blog/est3/","result":{"data":{"site":{"siteMetadata":{"title":"K011"}},"mdx":{"id":"5fd502dd-1520-5154-8bac-822ce87af9d7","body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Regression and Prediction\",\n  \"date\": \"2021-11-02\",\n  \"description\": \"REGRESSION\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h2\", null, \"Simple linear Regression\"), mdx(\"p\", null, \"Simple Linear Regression Simple linear regression provides a model of the relationship between the magnitude of one variable and that of a second for example, as X increases, Y also increases. Or as X increases, Y decreases.1 Correlation is another way to measure how two variables are related see. Stimates how much Y will change when X changes by a certain amount. With the correlation coefficient, the variables X and Y are interchangeable, With regression, we are trying to predict the Y variable from X using a linear relationship\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"Y_{i}=b_{0}+b_{1}X_{i}+e_{i}\"), \"\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"Y_{i}=b_{0}+b_{1}X_{i}+e_{i}\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Response\"), \" The variable we are trying to predict.\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Synonyms dependent variable, Y variable, target, outcome\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Independent variable\"), \" The variable used to predict the response.\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Synonyms X variable, feature, attribute, predictor\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Record\"), \" The vector of predictor and outcome values for a specific individual or case.\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Synonyms row, case, instance, example\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Intercept\"), \" The intercept of the regression line\\u2014that is, the predicted value when X = 0.\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Synonyms b0, \\u03B20\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Regression coefficient\"), \" The slope of the regression line.\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Synonyms slope, b1, \\u03B21, parameter estimates, weights\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fitted values\"), \" The estimates Yi obtained from the regression line.\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Synonym predicted values\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Residuals\"), \" The difference between the observed values and the fitted values.\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Synonym errors\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Least squares\"), \" The method of fitting a regression by minimizing the sum of squared residuals.\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Synonym Ordinary least squares, OLS\")))), mdx(\"h2\", null, \"Multiple linear Regression\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Least squares\"), \" The method of fitting a regression by minimizing the sum of squared residuals.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Root mean squared error\"), \"\\nThe square root of the average squared error of the regression (this is the most widely used metric to compare regression models).\\nSynonym RMSE\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Residual standard error\"), \"\\nThe same as the root mean squared error, but adjusted for degrees of freedom. Synonym RSE\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"R-squared\"), \"\\nThe proportion of variance explained by the model, from 0 to 1.\\nSynonyms coefficient of determination, R2\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"t-statistic\"), \"\\nThe coefficient for a predictor, divided by the standard error of the coefficient, giving a metric to compare the importance of variables in the model. See \\u201Ct-Tests\\u201D on page 110.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Weighted regression\"), \"\\nRegression with the records having different weights.\"))), mdx(\"p\", null, \"The primary purpose of regression in data science is prediction. This is useful to keep in mind, since regression, being an old and established statistical method, comes with baggage that is more relevant to its traditional role as a tool for explanatory modeling\"), mdx(\"h2\", null, \"Factor Variables\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dummy variables\"), \"\\nBinary 0\\u20131 variables derived by recoding factor data for use in regression and other models. \")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reference coding\"), \"\\nThe most common type of coding used by statisticians, in which one level of a factor is used as a reference and other factors are compared to that level. Synonym treatment coding \")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"One hot encoder\"), \"\\nA common type of coding used in the machine learning community in which all factor levels are retained. While useful for certain machine learning algorithms, this approach is not appropriate for multiple linear regression. \")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deviation coding \"), \"\\nA type of coding that compares each level against the overall mean as opposed to the reference level. Synonym sum contrasts Dummy Variables Representation\"))), mdx(\"h2\", null, \"Interpreting Regression Equation\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Correlated variables \"), \"\\nWhen the predictor variables are highly correlated, it is difficult to interpret the individual coefficients. \")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multicollinearity\"), \"\\nWhen the predictor variables have perfect, or near-perfect, correlation, the regression can be unstable or impossible to compute. Synonym collinearity \")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Confounding variables \"), \"\\nAn important predictor that, when omitted, leads to spurious relationships in a regression equation. \")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Main effects \"), \"\\nThe relationship between a predictor and the outcome variable, independent of other variables. \")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Interactions \"), \"\\nAn interdependent relationship between two or more predictors and the\"))), mdx(\"h2\", null, \"Regression Diagnostics\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Standardized residuals \"), \"\\nResiduals divided by the standard error of the residuals. \")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Outliers \"), \"\\nRecords (or outcome values) that are distant from the rest of the data (or the pre\\u2010 dicted outcome). \")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Influential value \"), \"\\nA value or record whose presence or absence makes a big difference in the regression equation. \")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Leverage \"), \"\\nThe degree of influence that a single record has on a regression equation. Synonym hat-value \")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Non-normal residuals \"), \"\\nNon-normally distributed residuals can invalidate some technical requirements of regression but are usually not a concern in data science. \")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Heteroskedasticity \"), \"\\nWhen some ranges of the outcome experience residuals with higher variance (may indicate a predictor missing from the equation). \")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Partial residual plots\"), \"\\nA diagnostic plot to illuminate the relationship between the outcome variable\"))), mdx(\"h2\", null, \"Polynomial and Spline Regression\"), mdx(\"p\", null, \"Polynomial regression involves including polynomial terms in a regression equation. The use of polynomial regression dates back almost to the development of regression itself with a paper by Gergonne in 1815. For example, a quadratic regression between\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Polynomial regression \"), \"\\nAdds polynomial terms (squares, cubes, etc.) to a regression. \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Spline regression \"), \"\\nFitting a smooth curve with a series of polynomial segments. \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Knots \"), \"\\nValues that separate spline segments. \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Generalized additive models \"), \"\\nSpline models with automated selection of knots. Synonym GAM\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-cpp\"\n  }, \"predictors = ['SqFtTotLiving', 'SqFtLot', 'Bathrooms',  'Bedrooms', 'BldgGrade'] \\noutcome = 'AdjSalePrice'\\nX = house_98105[predictors].values \\ny = house_98105[outcome] \\n\\ngam = LinearGAM(s(0, n_splines=12) + l(1) + l(2) + l(3) + l(4))   \\ngam.gridsearch(X, y)\\n\")));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"title":"Regression and Prediction","date":"2021-11-02","description":"REGRESSION"}},"allMdx":{"edges":[{"node":{"frontmatter":{"title":"Classification","date":"2021-11-03","description":"Naive Bayes, Discriminant Analysis, Logistic Regression"},"fields":{"slug":"/blog/est4/"}}},{"node":{"frontmatter":{"title":"Regression and Prediction","date":"2021-11-02","description":"REGRESSION"},"fields":{"slug":"/blog/est3/"}}},{"node":{"frontmatter":{"title":"Distribution","date":"2021-11-01","description":"Data distributions"},"fields":{"slug":"/blog/est1/"}}},{"node":{"frontmatter":{"title":"Stats Testing","date":"2021-11-01","description":"Statistical Experiments and significance testing"},"fields":{"slug":"/blog/est2/"}}},{"node":{"frontmatter":{"title":"Recursion","date":"2021-08-26","description":"Algorithm recursion"},"fields":{"slug":"/blog/alg1/"}}}]}},"pageContext":{"slug":"/blog/est3/","id":"5fd502dd-1520-5154-8bac-822ce87af9d7"}},"staticQueryHashes":["63159454"]}