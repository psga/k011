---
title: Classification 
date: "2021-11-03"
description: "Naive Bayes, Discriminant Analysis, Logistic Regression"
---
# Classification
1. Establish a cutoff probability for the class of interest, above which we consider a record as belonging to that class. 

2. Estimate (with any model) the probability that a record belongs to the class of interest. 

3. If that probability is above the cutoff probability, assign the new record to the class of interest.


## Naive Bayes
Naive bayes works with categorical (factor) prefictors and outcomes.
It asks, "Within each outcome category, which predictor categories are most probable?"


```cpp
predictors = ['purpose_', 'home_', 'emp_len_']
outcome = 'outcome'
X = pd.get_dummies(loan_data[predictors], prefix='', prefix_sep='')
y = loan_data[outcome] 

naive_model = MultinomialNB(alpha=0.01, fit_prior=True) 
naive_model.fit(X, y)
```
## Discriminant Analysis 
Discriminant analysis works with continuous or categorical predictors, as well as with categorical outcomes.Using the covariance matrix, it calculates a linear discriminant function, which is used to distinguish records belonging to one class from those belonging to another. 

This function is applied to the records to derive weights, or scores, for each record (one weight for each possible class), which determines its estimated class.
 
while the text and example in this section used just two predictor variables, LDA works just as well with more than two predictor variables. The only limiting factor is the num‐ ber of records (estimating the covariance matrix requires a suffi‐ cient number of records per variable, which is typically not an issue in data science applications). There are other variants of discriminant analysis. The best known is quadratic discriminant analysis (QDA). Despite its name, QDA is still a linear discriminant function. The main difference is that in LDA, the covariance matrix is assumed to be the same for the two groups corresponding to Y = 0 and Y = 1. In QDA, the covariance matrix is allowed to be different for the two groups. In practice, theº

## Logistic Regression
Logistic regression is analogous to multiple linear regression (see Chapter 4), except the outcome is binary. Various transformations are employed to convert the problem to one in which a linear model can be fit. Like discriminant analysis, and unlike K- Nearest Neighbor and naive Bayes, logistic regression is a structured model approach rather than a data-centric approach. Due to its fast computational speed and its output of a model that lends itself to rapid scoring of new data, it is a popular method.

The key ingredients for logistic regression are the logistic response function and the logit, in which we map a probability (which is on a 0–1 scale) to a more expansive scale suitable for linear modeling. 

### GLM

Generalized Linear Models Generalized linear models (GLMs) are characterized by two main components: 
- A probability distribution or family (binomial in the case of logistic regression) 
- A link function—i.e., a transformation function that maps the response to the predictors (logit in the case of logistic regression)

Logistic regression is by far the most common form of GLM. A data scientist will encounter other types of GLMs. Sometimes a log link function is used instead of the logit; in practice, use of a log link is unlikely to lead to very different results for most applications. The Poisson distribution is commonly used to model count data (e.g., the number of times a user visits a web page in a certain amount of time). Other fam‐ ilies include negative binomial and gamma, often used to model elapsed time (e.g., time to failure). In contrast to logistic regression, application of GLMs with these models is more nuanced and involves greater care. These are best avoided unless you are familiar with and understand the utility and pitfalls of these methods. Predicted Values from Logistic Regression

### Analysis of residuals
Analysis of Residuals’ is a mathematical method for checking if a regression model is a ‘good fit’.
